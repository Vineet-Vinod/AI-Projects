{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1bca178c",
      "metadata": {
        "id": "1bca178c"
      },
      "outputs": [],
      "source": [
        "!mkdir faces_positive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "cdc59b74",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdc59b74",
        "outputId": "9322ef3e-56c6-4888-d30e-6aa86570e265"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Using Colab cache for faster access to the 'human-faces' dataset.\n",
            "Path to dataset files: /kaggle/input/human-faces\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from google.colab import files\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms import ToTensor, RandomCrop, Normalize, Resize, Compose\n",
        "from tqdm import trange\n",
        "\n",
        "# --- Setup ---\n",
        "torch.manual_seed(15)\n",
        "random.seed(15)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "path = kagglehub.dataset_download(\"ashwingupta3012/human-faces\")\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1f2dca93",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f2dca93",
        "outputId": "0970e37f-21f2-4b42-895f-44eb1847b76d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 52%|█████▏    | 3763/7219 [02:19<02:11, 26.34it/s]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "100%|██████████| 7219/7219 [04:26<00:00, 27.08it/s]\n",
            "100%|██████████| 435/435 [00:01<00:00, 330.86it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean: (0.5227308869361877, 0.46569907665252686, 0.4313724637031555)\n",
            "Std. Dev: (0.3161585273687047, 0.3008185772213572, 0.2996168262495128)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Find mean and stddev of dataset\n",
        "NEGATIVES = os.path.join(path, 'Humans')\n",
        "POSITIVES = 'faces_positive'\n",
        "\n",
        "pixel_r = pixel_r2 = pixel_g = pixel_g2 = pixel_b = pixel_b2 = N = 0\n",
        "tt_obj = ToTensor()\n",
        "\n",
        "NEG_IMAGES = [os.path.join(NEGATIVES, img) for img in os.listdir(NEGATIVES)]\n",
        "remove_idxs = []\n",
        "for i in trange(len(NEG_IMAGES)):\n",
        "  img = NEG_IMAGES[i]\n",
        "\n",
        "  try:\n",
        "    image = Image.open(img).convert('RGB')\n",
        "  except:\n",
        "    remove_idxs.append(i)\n",
        "    continue\n",
        "\n",
        "  image_tensor = tt_obj(image)\n",
        "  if image_tensor.shape[0] != 3:\n",
        "    remove_idxs.append(i)\n",
        "    continue\n",
        "\n",
        "  N += image_tensor.shape[1] * image_tensor.shape[2]\n",
        "  pixel_r += torch.sum(image_tensor[0])\n",
        "  pixel_r2 += torch.sum(image_tensor[0] ** 2)\n",
        "  pixel_g += torch.sum(image_tensor[1])\n",
        "  pixel_g2 += torch.sum(image_tensor[1] ** 2)\n",
        "  pixel_b += torch.sum(image_tensor[2])\n",
        "  pixel_b2 += torch.sum(image_tensor[2] ** 2)\n",
        "\n",
        "POS_IMAGES = [os.path.join(POSITIVES, img) for img in os.listdir(POSITIVES)]\n",
        "for i in trange(len(POS_IMAGES)):\n",
        "  img = POS_IMAGES[i]\n",
        "  image = Image.open(img).convert('RGB')\n",
        "  image_tensor = tt_obj(image)\n",
        "\n",
        "  N += image_tensor.shape[1] * image_tensor.shape[2]\n",
        "  pixel_r += torch.sum(image_tensor[0])\n",
        "  pixel_r2 += torch.sum(image_tensor[0] ** 2)\n",
        "  pixel_g += torch.sum(image_tensor[1])\n",
        "  pixel_g2 += torch.sum(image_tensor[1] ** 2)\n",
        "  pixel_b += torch.sum(image_tensor[2])\n",
        "  pixel_b2 += torch.sum(image_tensor[2] ** 2)\n",
        "\n",
        "mean = (float(pixel_r / N), float(pixel_g / N), float(pixel_b / N))\n",
        "stddev = (float(pixel_r2 / N - mean[0] ** 2) ** 0.5, float(pixel_g2 / N - mean[1] ** 2) ** 0.5, float(pixel_b2 / N - mean[2] ** 2) ** 0.5)\n",
        "print(f'Mean: {mean}\\nStd. Dev: {stddev}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c11bb3f2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c11bb3f2",
        "outputId": "8c611c43-5d02-48ac-8ec0-be7fa296462c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Removing 0 images from the negatives dataset (incompatible number of channels)\n"
          ]
        }
      ],
      "source": [
        "# Clean up negatives dataset\n",
        "print(f'Removing {len(remove_idxs)} images from the negatives dataset (incompatible number of channels)')\n",
        "remove_idxs.reverse()\n",
        "for idx in remove_idxs:\n",
        "    NEG_IMAGES.pop(idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "30ef2588",
      "metadata": {
        "id": "30ef2588"
      },
      "outputs": [],
      "source": [
        "# Data transforms\n",
        "IMAGE_SIZE = 224\n",
        "TRAIN_TEST_SPLIT = 0.8\n",
        "\n",
        "transform_train = Compose([\n",
        "    Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    RandomCrop(IMAGE_SIZE, padding=4),\n",
        "    ToTensor(),\n",
        "    Normalize(mean, stddev),\n",
        "])\n",
        "\n",
        "transform_test = Compose([\n",
        "    Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    ToTensor(),\n",
        "    Normalize(mean, stddev),\n",
        "])\n",
        "\n",
        "class FaceDataset(Dataset):\n",
        "    def __init__(self, positives, negatives, transform):\n",
        "        self.images = [(p, 1) for p in positives] + [(n, 0) for n in negatives]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.images[idx]\n",
        "        image = Image.open(path).convert('RGB')\n",
        "\n",
        "        return self.transform(image), label\n",
        "\n",
        "\n",
        "random.shuffle(NEG_IMAGES)\n",
        "random.shuffle(POS_IMAGES)\n",
        "train_neg, test_neg = NEG_IMAGES[:int(len(NEG_IMAGES)*TRAIN_TEST_SPLIT)], NEG_IMAGES[int(len(NEG_IMAGES)*TRAIN_TEST_SPLIT):]\n",
        "train_pos, test_pos = POS_IMAGES[:int(len(POS_IMAGES)*TRAIN_TEST_SPLIT)], POS_IMAGES[int(len(POS_IMAGES)*TRAIN_TEST_SPLIT):]\n",
        "train_dataset = FaceDataset(train_pos, train_neg, transform_train)\n",
        "test_dataset = FaceDataset(test_pos, test_neg, transform_test)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "5de8911e",
      "metadata": {
        "id": "5de8911e"
      },
      "outputs": [],
      "source": [
        "# Helpers\n",
        "def train_model(model, train_loader, epochs=10, learning_rate=1e-3):\n",
        "    print(f\"--- Starting Training for {model.__class__.__name__} ---\")\n",
        "\n",
        "    model.to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in trange(epochs):\n",
        "        start_time = time.time()\n",
        "        running_loss = 0.0\n",
        "        model.train()\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels.float().view(-1, 1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        epoch_time = time.time() - start_time\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {epoch_loss:.4f}, Time: {epoch_time:.2f}s\")\n",
        "\n",
        "    print(\"--- Finished Training ---\\n\")\n",
        "    return model\n",
        "\n",
        "def test_model(model, test_loader):\n",
        "    print(f\"--- Starting Testing for {model.__class__.__name__} ---\")\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            print(images.shape)\n",
        "            outputs = model(images)\n",
        "            predicted = (outputs.data > 0).int()\n",
        "            print(predicted[0], labels.view(-1, 1)[0])\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels.view(-1, 1)).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Test accuracy of model: {accuracy:.1f}%\")\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1137cb4a",
      "metadata": {
        "id": "1137cb4a"
      },
      "outputs": [],
      "source": [
        "# Model Definition\n",
        "class FaceRecognition(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            # Block 1\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2), # 224->112\n",
        "            # Block 2\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2), # 112->56\n",
        "            # Block 3\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2), # 56->28\n",
        "            # Block 4\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2), # 28->14\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256 * 14 * 14, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(1024, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f461f86f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "f461f86f",
        "outputId": "641c8119-80e6-45fc-daaa-98e048ba8dbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Starting Training for FaceRecognition ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 1/5 [02:35<10:23, 155.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5, Training Loss: 0.4400, Time: 155.99s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 2/5 [05:12<07:48, 156.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/5, Training Loss: 0.0014, Time: 156.28s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 3/5 [07:48<05:11, 155.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/5, Training Loss: 0.0000, Time: 155.76s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 4/5 [10:23<02:35, 155.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/5, Training Loss: 0.0000, Time: 155.77s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [12:59<00:00, 155.86s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/5, Training Loss: 0.0000, Time: 155.50s\n",
            "--- Finished Training ---\n"
          ]
        }
      ],
      "source": [
        "fr_model = FaceRecognition()\n",
        "trained_cnn = train_model(fr_model, train_loader, epochs=5) # Could use just 3-4 epochs..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "kYmdkoRWkU43",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYmdkoRWkU43",
        "outputId": "69628345-f849-4011-fb6f-066625f3eb8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Starting Testing for FaceRecognition ---\n",
            "torch.Size([64, 3, 224, 224])\n",
            "tensor([1], device='cuda:0', dtype=torch.int32) tensor([1], device='cuda:0')\n",
            "torch.Size([64, 3, 224, 224])\n",
            "tensor([1], device='cuda:0', dtype=torch.int32) tensor([1], device='cuda:0')\n",
            "torch.Size([64, 3, 224, 224])\n",
            "tensor([0], device='cuda:0', dtype=torch.int32) tensor([0], device='cuda:0')\n",
            "torch.Size([64, 3, 224, 224])\n",
            "tensor([0], device='cuda:0', dtype=torch.int32) tensor([0], device='cuda:0')\n",
            "torch.Size([64, 3, 224, 224])\n",
            "tensor([0], device='cuda:0', dtype=torch.int32) tensor([0], device='cuda:0')\n",
            "torch.Size([64, 3, 224, 224])\n",
            "tensor([0], device='cuda:0', dtype=torch.int32) tensor([0], device='cuda:0')\n",
            "torch.Size([64, 3, 224, 224])\n",
            "tensor([0], device='cuda:0', dtype=torch.int32) tensor([0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 3, 224, 224])\n",
            "tensor([0], device='cuda:0', dtype=torch.int32) tensor([0], device='cuda:0')\n",
            "torch.Size([64, 3, 224, 224])\n",
            "tensor([0], device='cuda:0', dtype=torch.int32) tensor([0], device='cuda:0')\n",
            "torch.Size([64, 3, 224, 224])\n",
            "tensor([0], device='cuda:0', dtype=torch.int32) tensor([0], device='cuda:0')\n",
            "torch.Size([64, 3, 224, 224])\n",
            "tensor([0], device='cuda:0', dtype=torch.int32) tensor([0], device='cuda:0')\n",
            "torch.Size([64, 3, 224, 224])\n",
            "tensor([0], device='cuda:0', dtype=torch.int32) tensor([0], device='cuda:0')\n",
            "torch.Size([64, 3, 224, 224])\n",
            "tensor([0], device='cuda:0', dtype=torch.int32) tensor([0], device='cuda:0')\n",
            "torch.Size([64, 3, 224, 224])\n",
            "tensor([0], device='cuda:0', dtype=torch.int32) tensor([0], device='cuda:0')\n",
            "torch.Size([64, 3, 224, 224])\n",
            "tensor([0], device='cuda:0', dtype=torch.int32) tensor([0], device='cuda:0')\n",
            "torch.Size([64, 3, 224, 224])\n",
            "tensor([0], device='cuda:0', dtype=torch.int32) tensor([0], device='cuda:0')\n",
            "torch.Size([64, 3, 224, 224])\n",
            "tensor([0], device='cuda:0', dtype=torch.int32) tensor([0], device='cuda:0')\n",
            "torch.Size([64, 3, 224, 224])\n",
            "tensor([0], device='cuda:0', dtype=torch.int32) tensor([0], device='cuda:0')\n",
            "torch.Size([64, 3, 224, 224])\n",
            "tensor([0], device='cuda:0', dtype=torch.int32) tensor([0], device='cuda:0')\n",
            "torch.Size([64, 3, 224, 224])\n",
            "tensor([0], device='cuda:0', dtype=torch.int32) tensor([0], device='cuda:0')\n",
            "torch.Size([64, 3, 224, 224])\n",
            "tensor([0], device='cuda:0', dtype=torch.int32) tensor([0], device='cuda:0')\n",
            "torch.Size([64, 3, 224, 224])\n",
            "tensor([0], device='cuda:0', dtype=torch.int32) tensor([0], device='cuda:0')\n",
            "torch.Size([64, 3, 224, 224])\n",
            "tensor([0], device='cuda:0', dtype=torch.int32) tensor([0], device='cuda:0')\n",
            "torch.Size([59, 3, 224, 224])\n",
            "tensor([0], device='cuda:0', dtype=torch.int32) tensor([0], device='cuda:0')\n",
            "Test accuracy of model: 100.0%\n"
          ]
        }
      ],
      "source": [
        "cnn_accuracy = test_model(trained_cnn, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "NEb4mROJdGw8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "NEb4mROJdGw8",
        "outputId": "e409d854-1c77-42e0-de28-f7b1f33f09ec"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_2dca42cb-956e-4655-9d46-d9b625de1ac5\", \"face_recognition_model.pth\", 207102333)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "MODEL_PATH = 'face_recognition_model.pth'\n",
        "torch.save(trained_cnn.state_dict(), MODEL_PATH)\n",
        "files.download(MODEL_PATH)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
